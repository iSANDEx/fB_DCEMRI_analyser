{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iSANDEx/fB_DCEMRI_analyser/blob/main/jupyter/GoogleColab_RegistrationWorkflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zKaOP2dsFj4h",
      "metadata": {
        "id": "zKaOP2dsFj4h"
      },
      "source": [
        "# Data Analysis Notebook\n",
        "This notebook allows you to analyse functional Breast DCE-MRI datasets, starting from the raw DICOM files up to displaying time course signal intensity curves showing tumour uptake of contrast agent.\n",
        "\n",
        "Although most of the folder structure and data processing is done automatically, there are some manual interactions that still need to happen.\n",
        "\n",
        "This notebook uses the standard (i.e. free) GCP resources, so registration might take longer than it'd take in a local machine (~1hr per patient). However, it can be left running and then come back to look at the results :)\n",
        "\n",
        "Compared to set it up in a local machine, it doesn't require to install anything nor setup a local development environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xw8u6g-_FlED",
      "metadata": {
        "id": "Xw8u6g-_FlED"
      },
      "source": [
        "# Setting up the running environment\n",
        "These set of cells will prepare the environment to run the registration workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2vxSYiP6Fqke",
      "metadata": {
        "id": "2vxSYiP6Fqke"
      },
      "source": [
        "## Installation and Loading Modules\n",
        "It installs specific tools to manage medical images, DICOM data as well as tools for visualisation and display:\n",
        "* [Matplotlib](https://matplotlib.org/)\n",
        "* [PyDICOM](https://pydicom.github.io/pydicom/stable/index.html)\n",
        "* [DICOM2NIFTI](https://github.com/icometrix/dicom2nifti)\n",
        "* [directory_tree](https://github.com/rahulbordoloi/Directory-Tree/)\n",
        "* [ITK](https://github.com/InsightSoftwareConsortium/ITKPythonPackage)\n",
        "* [ANTs](https://github.com/ANTsX/ANTsPy)\n",
        "* [ITK-Elastix](https://pypi.org/project/itk-elastix/)\n",
        "* [OpenCV](https://opencv.org/releases/)\n",
        "* [Roifile](https://pypi.org/project/roifile/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wlsD_PQf-3O2",
      "metadata": {
        "id": "wlsD_PQf-3O2"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib pydicom dicom2nifti directory_tree itk ants itk-elastix opencv-python roifile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4382f573-24ad-4d1c-b724-39c54384a77f",
      "metadata": {
        "id": "4382f573-24ad-4d1c-b724-39c54384a77f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Load additional Python modules:\n",
        "import os\n",
        "import cv2 # Finally I can come back to OpenCV!!!\n",
        "import itk\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import ants\n",
        "import json\n",
        "import glob\n",
        "import shutil\n",
        "import roifile  # Opening the ImageJ-created ROIs (https://pypi.org/project/roifile/)\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dicom2nifti\n",
        "import pydicom as pyd\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "from directory_tree import display_tree  # Nice tool to display directory trees (https://pypi.org/project/directory-tree/)\n",
        "\n",
        "print('All external modules loaded. Please check above for any error message!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "476f62b0-67a7-4a8b-82a2-6274d9562793",
      "metadata": {
        "id": "476f62b0-67a7-4a8b-82a2-6274d9562793",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Auxiliar functions\n",
        "def list_folder_content(path, show_hidden=False):\n",
        "    if show_hidden:\n",
        "        ddfldrlst = os.listdir(path)\n",
        "    else:\n",
        "        ddfldrlst = list(filter(lambda item: not item.startswith('.'),os.listdir(path)))\n",
        "    return ddfldrlst\n",
        "\n",
        "def display_folder_list(file_list):\n",
        "    print('\\n'.join(f'[{idx}] - {file_idx}' for idx, file_idx in enumerate(file_list)))\n",
        "\n",
        "def get_path_to_process(full_path):\n",
        "    print('Folder content:')\n",
        "    print(display_tree(full_path, header=True, string_rep=True, show_hidden=False, max_depth=2))\n",
        "    folder_content = list_folder_content(full_path)\n",
        "    # Ideally we'll have only one sub-folder inside the PreTreatment folder. If more than one, then we have to choose, but by default, we'll select the first one.\n",
        "    idx_reg = 0\n",
        "    if len(folder_content) > 1:\n",
        "        display_folder_list(folder_content)\n",
        "        idx_sel = input(f'Select the folder with the dataset_to_process to process (0-{len(folder_content)-1} or just press Enter to proceed with sub-folder {folder_content[idx_reg]}):')\n",
        "        if idx_sel:\n",
        "            idx_reg = int(idx_sel)\n",
        "    path2data = os.path.join(full_path, folder_content[idx_reg])\n",
        "    print(f'Will process {folder_content[idx_reg]}')\n",
        "    return path2data\n",
        "\n",
        "def check_time_points(path_to_check, nmax = 6, verbose=False):\n",
        "    if path_to_check is not None:\n",
        "        nr_of_folders = list_folder_content(path_to_check)\n",
        "        print(f'Folder {path_to_check} seems Ok' if len(nr_of_folders)== nmax else f'Error! Check path {path_to_check} is the correct one')\n",
        "        if verbose:\n",
        "            print('Listing folder content:')\n",
        "            display_tree(path_to_check, max_depth=1)\n",
        "        return nr_of_folders if len(nr_of_folders) == nmax else None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def add_prefix_to_filename(full_path, prefix=None):\n",
        "    # Assume the last part of the path is the filename (with extension)\n",
        "    file_path, file_name_ext = os.path.split(full_path)\n",
        "    if prefix:\n",
        "        updated_filename = '_'.join([prefix, file_name_ext])\n",
        "        return os.path.join(file_path, updated_filename)\n",
        "    else:\n",
        "        return prefix\n",
        "\n",
        "def itk2ants(itkInput):\n",
        "    antsOutput = ants.from_numpy(itk.GetArrayFromImage(itkInput).T,\n",
        "                                 origin=tuple(itkInput.GetOrigin()),\n",
        "                                 spacing=tuple(itkInput.GetSpacing()),\n",
        "                                 direction=np.array(itkInput.GetDirection()))\n",
        "\n",
        "    return antsOutput\n",
        "\n",
        "def ants2itk(antsInput):\n",
        "    itkOutput = itk.GetImageFromArray(antsInput.numpy().T)\n",
        "    itkOutput.SetOrigin(antsInput.origin)\n",
        "    itkOutput.SetSpacing(antsInput.spacing)\n",
        "    itkOutput.SetDirection(antsInput.direction)\n",
        "\n",
        "    return itkOutput\n",
        "\n",
        "def getenv(colab=False):\n",
        "    \"\"\"\n",
        "    Requires sys and os modules:\n",
        "    import sys\n",
        "    import os\n",
        "    Possible values for sys.platform are (https://docs.python.org/3/library/sys.html & https://stackoverflow.com/questions/446209/possible-values-from-sys-platform)\n",
        "    ┍━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━┑\n",
        "    │  System             │ Value               │\n",
        "    ┝━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━┥\n",
        "    │ Linux               │ linux or linux2 (*) │\n",
        "    │ Windows             │ win32               │\n",
        "    │ Windows/Cygwin      │ cygwin              │\n",
        "    │ Windows/MSYS2       │ msys                │\n",
        "    │ Mac OS X            │ darwin              │\n",
        "    │ OS/2                │ os2                 │\n",
        "    │ OS/2 EMX            │ os2emx              │\n",
        "    │ RiscOS              │ riscos              │\n",
        "    │ AtheOS              │ atheos              │\n",
        "    │ FreeBSD 7           │ freebsd7            │\n",
        "    │ FreeBSD 8           │ freebsd8            │\n",
        "    │ FreeBSD N           │ freebsdN            │\n",
        "    │ OpenBSD 6           │ openbsd6            │\n",
        "    │ AIX                 │ aix (**)            │\n",
        "    ┕━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━┙\n",
        "    Update 03/04/2024: Added support when running inside Google Colab Environment, via the optional flag \"colab\"\n",
        "    \"\"\"\n",
        "    if colab:\n",
        "      # Requires mounting the Google Drive before\n",
        "      HOMEPATH = '/content/drive/MyDrive'\n",
        "    else:\n",
        "      if sys.platform == 'win32':\n",
        "          env_home = 'HOMEPATH'\n",
        "      elif (sys.platform == 'darwin') | (sys.platform == 'linux'):\n",
        "          env_home = 'HOME'\n",
        "      HOMEPATH = os.getenv(env_home)\n",
        "\n",
        "    return HOMEPATH\n",
        "\n",
        "def check_path_exist(path, file=False):\n",
        "    \"\"\"\n",
        "    Flag FILE indicates the path contains a file name (FLAG=TRUE) or the path only points to a folder (FLAG=FALSE (Default))\n",
        "    \"\"\"\n",
        "    if file:\n",
        "        is_path = os.path.isfile(path)\n",
        "    else:\n",
        "        is_path = os.path.isdir(path)\n",
        "\n",
        "    print(f'{\"OK:\" if is_path else \"ERROR:\"} Path to {\"file\" if file else \"folder\"} {path} does{\"\" if is_path else \" NOT\"} exist')\n",
        "\n",
        "    return is_path\n",
        "\n",
        "def create_folder_structure(root_path, sub_dirs, create=True):\n",
        "  \"\"\"\n",
        "  Create subdirectories insode HOMEPATH (using makedirs to avoid altering existing folders)\n",
        "  ROOT_PATH must exist, otherwise it issues an error\n",
        "  SUB_DIRS is a dictionary with the subfolder required inside HOMEPATH\n",
        "\n",
        "  \"\"\"\n",
        "  subdirpaths = {}\n",
        "  for folder_name, sub_dir in sub_dirs.items():\n",
        "    path_to_subdir = os.path.join(root_path, sub_dir)\n",
        "    if create:\n",
        "        os.makedirs(path_to_subdir, exist_ok=True)\n",
        "        subdirpaths[folder_name] = path_to_subdir\n",
        "\n",
        "\n",
        "  return subdirpaths\n",
        "print('All auxiliar functions defined. Ready to proceed')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OHSCG6CMF3Xs",
      "metadata": {
        "id": "OHSCG6CMF3Xs"
      },
      "source": [
        "## Path to Data and Google Drive\n",
        "Mount the user-owned Google Drive to access the DICOM data. It requires access through the google account, but none of the access credentials are stored in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q10Plz3DF7u1",
      "metadata": {
        "id": "q10Plz3DF7u1"
      },
      "outputs": [],
      "source": [
        "# @title Mount the user-owned Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "be4383db-cc72-43bf-b968-9411cbf170b3",
      "metadata": {
        "id": "be4383db-cc72-43bf-b968-9411cbf170b3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Create the Folder Structure\n",
        "# (This is slightly different to the one created for testing registration methods, but still follows the same criteria)\n",
        "HOMEPATH = getenv(colab=True)\n",
        "ROOTPATH = os.path.join(HOMEPATH, 'Data', 'fMRIBreastData')\n",
        "\n",
        "SUBDIRFLDRS = {'STUDYFLDR':   'StudyData',\n",
        "               'NIFTIFLDR':   'NiftiData',\n",
        "               'ROIFLDR':     'ROIData',\n",
        "               'SRCDCMFLDR':  'rawS3',\n",
        "               'REGOUTFLDR':  'RegOutput',\n",
        "               'CFGFLDR':     'configFiles',\n",
        "               'OUTPUTFLDR':  'output',\n",
        "               'LOGFLDR':     'logs'\n",
        "              }\n",
        "subdir_paths = create_folder_structure(ROOTPATH, SUBDIRFLDRS)\n",
        "\n",
        "# DICOM Sources:\n",
        "srcpath = subdir_paths['SRCDCMFLDR']\n",
        "dstpath = subdir_paths['STUDYFLDR']\n",
        "# Check the path exist and are correct:\n",
        "# check_path_exist(srcpath)\n",
        "# check_path_exist(dstpath)\n",
        "fext = 'dcm'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download configuration files and previously defined ROIs\n",
        "elastix_model_zoo_path = 'https://github.com/SuperElastix/ElastixModelZoo/blob/master/models/Par0032/'\n",
        "elastix_pars = ['Par0032_bsplines.txt','Par0032_rigid.txt']\n",
        "for elastix_par in elastix_pars:\n",
        "  if os.path.isfile(os.path.join(subdir_paths['CFGFLDR'], elastix_par)):\n",
        "    print(f'File {elastix_par} already exist, nothing done')\n",
        "  else:\n",
        "    path_to_elastix_cfgfile = '/'.join([elastix_model_zoo_path, elastix_par])\n",
        "    !wget {path_to_elastix_cfgfile} -P {subdir_paths['CFGFLDR']}\n",
        "    print(f\"File {elastix_par} saved at {subdir_paths['CFGFLDR']}\")"
      ],
      "metadata": {
        "id": "Gp3vFyhczPzh"
      },
      "id": "Gp3vFyhczPzh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "JfwMzUQ_Xq4s",
      "metadata": {
        "id": "JfwMzUQ_Xq4s"
      },
      "source": [
        "Now that the folders have been created, (manually) place the dicom (raw) data into the ```rawS3``` folder (or whatever value is in ```subdir_paths['SRCDCMFLDR']```)\n",
        "\n",
        "After done that, we'll sort them out automatically in the ```subdir_paths['STUDYFLDR']```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5CxOaAQMeVPV",
      "metadata": {
        "id": "5CxOaAQMeVPV"
      },
      "source": [
        "# Sort out DICOM folders\n",
        "Sort out dicom files following this order:\n",
        "* PatientID () --> Not used\n",
        "* PatientName () --> Use this as it is also used by Radiologists, so to keep consistency\n",
        "* StudyID () --> Not needed\n",
        "* StudyDate () --> This defines the pre- and post-treatment (just the earliest is pre- and the latest is post-)\n",
        "* SeriesNro () --> This is relevant to later eliminate any possible in-scanner post-processed data\n",
        "* SeriesDescription ()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bU28BPK0ePCe",
      "metadata": {
        "id": "bU28BPK0ePCe"
      },
      "outputs": [],
      "source": [
        "# @title Get the list of dicom files in the rawdata folder\n",
        "# (use the extension defined by FEXT)\n",
        "dcmlist = glob.glob(os.path.join(srcpath,f'*.{fext}'))\n",
        "print(f'There are {len(dcmlist)} files to process in {srcpath}. Please wait...')\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "nels = len(dcmlist)\n",
        "nsteps = 10**math.floor(math.log10(0.01*nels)+1)\n",
        "\n",
        "print(''.join(['*']*100))\n",
        "for nimg, dcm in enumerate(dcmlist):\n",
        "    if (nimg % nsteps)==0:\n",
        "        print(f'\\t{nels-nimg} files to process...')\n",
        "\n",
        "    ds = pyd.dcmread(dcm,stop_before_pixels=True)\n",
        "    [PatientID,\n",
        "     PatientName,\n",
        "     StudyDate,\n",
        "     SeriesNro,\n",
        "     TempPos] = [ds.PatientID, str(ds.PatientName), ds.StudyDate,\n",
        "                 str(ds.SeriesNumber), str(ds.TemporalPositionIdentifier)]\n",
        "\n",
        "    name_as_list = PatientName.split(' ')\n",
        "    # remove multiple spaces:\n",
        "    name_no_space = [i for i in name_as_list if i != '']\n",
        "    # From the second element onward, use camel-case:\n",
        "    name_camel_case = [i.title().replace('Treatmensst','Treatment') if idx>0 else i for idx, i in enumerate(name_no_space)]\n",
        "    # Re-Join the name with a dash instead of (multiple) spaces:\n",
        "    PatientName = '-'.join(name_camel_case)\n",
        "    folderStruct = os.path.join(dstpath,\n",
        "                                '-'.join([PatientName[:PatientName.find('-')],\n",
        "                                          PatientID]),\n",
        "                                '-'.join([PatientName.replace(' ','_'),\n",
        "                                          StudyDate.replace(' ','_')]),\n",
        "                                SeriesNro.replace(' ','_'),\n",
        "                                TempPos.replace(' ','_'))\n",
        "    os.makedirs(folderStruct, exist_ok=True)\n",
        "    dstFile = os.path.join(folderStruct, os.path.split(dcm)[-1])\n",
        "    if not os.path.isfile(dstFile):\n",
        "        shutil.copy2(dcm, dstFile)\n",
        "    else:\n",
        "        print(f'File {dstFile}, already exists. Nothing done')\n",
        "end_time = time.perf_counter()\n",
        "elp_time = end_time - start_time\n",
        "print(''.join(['*']*100))\n",
        "print(f'All done (Elapsed time was {elp_time:.1f}[s]). Showing just the last folder processed:')\n",
        "print(f'{folderStruct}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wMx4xXdIeLQC",
      "metadata": {
        "id": "wMx4xXdIeLQC"
      },
      "source": [
        "# Convert DICOM to NIFTI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bT9ecqr_eLjl",
      "metadata": {
        "id": "bT9ecqr_eLjl"
      },
      "outputs": [],
      "source": [
        "# @title Display the folder tree after sorting the DICOM data\n",
        "dcmpath = subdir_paths['STUDYFLDR']\n",
        "niftipath = subdir_paths['NIFTIFLDR']\n",
        "# Check the path exist and are correct:\n",
        "# check_path_exist(dcmpath)\n",
        "# check_path_exist(niftipath)\n",
        "\n",
        "# Displays the content of the DICOM folders that will be converted to NIFTI:\n",
        "print(display_tree(dcmpath, header=True, string_rep=True, show_hidden=False, max_depth=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iE0cdYq8kkUF",
      "metadata": {
        "id": "iE0cdYq8kkUF"
      },
      "outputs": [],
      "source": [
        "# @title Mirror the folder structure and performs the conversion DICOM --> Nifti\n",
        "# Expected sub-folder tree follows the pattern:\n",
        "# PATIENT_INITIALS+\"-\"+PATIENTID -> PATIENT_NAME+\"-\"+DATE_OF_VISIT -> DCE_MRI_SEQUENCE\n",
        "# Valid PATIENT_NAMES contains at most 3 dashes (i.e. to exclude \"Motion Corrected\" one, but keeping \"RICE001\")\n",
        "#\n",
        "start_time = time.perf_counter()\n",
        "over_write = False # If the file exists, doesn't overwrite (i.e. doesn't convert it again)\n",
        "concat_vol = False\n",
        "PATIENTSID = list_folder_content(dcmpath)\n",
        "\n",
        "print(''.join(['*']*100))\n",
        "for patientID in PATIENTSID:\n",
        "    path2patientID = os.path.join(dcmpath, patientID)\n",
        "    patient_visits = list_folder_content(path2patientID)\n",
        "    for visit_name in patient_visits:\n",
        "        path2visit = os.path.join(path2patientID, visit_name)\n",
        "        if visit_name.count('-') > 3:\n",
        "            print(f'Folder Name is too long to match the criteria. Skipping {path2visit}')\n",
        "            continue\n",
        "        print(f'Processing {visit_name}. Please wait...')\n",
        "        seq_nro = os.path.join(path2visit, list_folder_content(path2visit)[0])\n",
        "        dce_time_points = list_folder_content(seq_nro)\n",
        "        if concat_vol:\n",
        "            tseries_volume = [None]*len(dce_time_points)\n",
        "        for time_point_i in dce_time_points:\n",
        "            path2time_point = os.path.join(seq_nro, time_point_i)\n",
        "            print(f'Replicating sub-folder \"{time_point_i}\" in the output directory {niftipath}:')\n",
        "            ipath = path2time_point\n",
        "            opath = ipath.replace(dcmpath, niftipath)\n",
        "            print(f'Creating folder {opath}...')\n",
        "            try:\n",
        "                os.makedirs(opath, exist_ok=True)\n",
        "            except Exception as err:\n",
        "                print(f'ERROR: Cannot create the folder {opath}')\n",
        "                break\n",
        "            if (not os.path.isdir(opath)) | over_write:\n",
        "              print(f'Performing conversion from {ipath}, please wait...')\n",
        "              dicom2nifti.convert_directory(ipath, opath)\n",
        "              if concat_vol:\n",
        "                  tseries_volume[int(time_point_i)-1] = itk.imread(glob.glob(os.path.join(opath,'*.nii.gz'))[0])\n",
        "            else:\n",
        "              print(f'File {opath}, already exists, nothing done. If you want to overwrite it, set the flag OVER_WRITE to TRUE')\n",
        "\n",
        "        if concat_vol:\n",
        "            print(f\"Conversion done successfully, now will concatenate the timeseries into a single 4D volume saved at {os.path.join(path2visit.replace(INPUTPATH, OUTPUTPATH),'.'.join([visit_name,'nii.gz']))}. Please wait a little bit more :) ...\")\n",
        "            # Initialise the 4d volume with the first time point (pre-contrast)\n",
        "            t0_nii = tseries_volume[0]\n",
        "            in_dim = t0_nii.GetImageDimension()\n",
        "            pixel_type = itk.template(t0_nii)[1][0]\n",
        "            out_dim = in_dim + 1\n",
        "\n",
        "            input_image_type = itk.Image[pixel_type, in_dim]\n",
        "            output_image_type = itk.Image[pixel_type, out_dim]\n",
        "\n",
        "            layout = [1, 1, 1, len(dce_time_points)]\n",
        "            vol_tiles = itk.TileImageFilter[input_image_type, output_image_type].New()\n",
        "            vol_tiles.SetLayout(layout)\n",
        "            for idx in range(len(dce_time_points)):\n",
        "                vol_tiles.SetInput(idx, tseries_volume[idx])\n",
        "            # Write 4D Volume:\n",
        "            volume_writer = itk.ImageFileWriter[output_image_type].New()\n",
        "            volume_writer.SetFileName(os.path.join(path2visit.replace(dcmpath, niftipath),'.'.join([visit_name,'nii.gz'])))\n",
        "            volume_writer.SetInput(vol_tiles.GetOutput())\n",
        "            volume_writer.Update()\n",
        "            # And removing useless data:\n",
        "            print(f'Removing {seq_nro} and its content...')\n",
        "            shutil.rmtree(seq_nro.replace(dcmpath, niftipath))\n",
        "        else:\n",
        "            print('Moving to the next data folder.')\n",
        "        print(''.join(['*']*100))\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "elp_time = end_time - start_time\n",
        "\n",
        "print(f'All done (Elapsed time was {elp_time:.1f}[s]), converted files have been saved at {dcmpath}. Bye!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VtQIBm5CmTUN",
      "metadata": {
        "id": "VtQIBm5CmTUN"
      },
      "source": [
        "# Register the Nifti datasets\n",
        "Here, we need to get the Elastix registration parameter files from the model zoo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab457145-d861-4fa2-9129-bbca3b8e2479",
      "metadata": {
        "id": "ab457145-d861-4fa2-9129-bbca3b8e2479",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Creates the dataset structure\n",
        "# (i.e. registration parameters and path to config files)\n",
        "savepath = subdir_paths['REGOUTFLDR']\n",
        "studypath = subdir_paths['NIFTIFLDR']\n",
        "\n",
        "\n",
        "configpath = subdir_paths['CFGFLDR']\n",
        "logpath = subdir_paths['LOGFLDR']\n",
        "\n",
        "# configpath = subdir_paths['CFGFLDR']\n",
        "# Check the path exist and are correct:\n",
        "# check_path_exist(dcmpath)\n",
        "# check_path_exist(niftipath)\n",
        "\n",
        "\n",
        "# default settings:\n",
        "DEBUGMODE = True\n",
        "BATCHMODE = False # TRUE: it runs the registration for all dataset within STUDYPATH; FALSE (DEFAULT): allows to pick a specific dataset to register\n",
        "\n",
        "registration_algorithm = 'Elastix'\n",
        "fixed_volume_pos = 2\n",
        "register_fixed = True\n",
        "config_files = ['Par0032_rigid.txt', 'Par0032_bsplines.txt']\n",
        "txt_description = 'Default registration (Stage 1) of functional Breast DCE-MRI datasets'\n",
        "platform = 'any'\n",
        "bias_correction = 'n4itk'\n",
        "histogram_matching = False\n",
        "\n",
        "\n",
        "dataset_to_process = {'study_path': studypath,\n",
        "                      'save_path': savepath,\n",
        "                      'data_path': os.path.join(savepath, 'datasets'),\n",
        "                      'parameters_folder': 'parameters',\n",
        "                      'intended_platform': platform,\n",
        "                      'run_platform': sys.platform,\n",
        "                      'registration_details': {'algorithm': registration_algorithm,\n",
        "                                               'configuration_files': config_files,\n",
        "                                               'register_fixed': register_fixed},\n",
        "                      'fixed_volume_position': fixed_volume_pos,\n",
        "                      'preprocessing': {'bias_correction': bias_correction,\n",
        "                                        'histogram_matching': histogram_matching},\n",
        "                      'datasets': {}\n",
        "                     }\n",
        "# Create the top level folder(s) inside SAVEPATH:\n",
        "os.makedirs(os.path.join(dataset_to_process['save_path'],dataset_to_process['parameters_folder']), exist_ok=True)\n",
        "\n",
        "# Create description.json file from the descriptive variables defined in the previous cell:\n",
        "description = {'Summary': txt_description,\n",
        "               'Intended Platform': platform,\n",
        "               'Run Platform': sys.platform,\n",
        "               'Registration Details': {'algorithm': registration_algorithm,\n",
        "                                        'configuration parameters': config_files,\n",
        "                                        'reference volume': fixed_volume_pos,\n",
        "                                        'register fixed': register_fixed},\n",
        "               'Preprocessing': {'bias_correction': bias_correction,\n",
        "                                 'histogram_matching': histogram_matching}\n",
        "               }\n",
        "\n",
        "# Save the description as a JSON file in the output directory:\n",
        "with open(os.path.join(dataset_to_process['save_path'], 'description.json'), 'w') as fp:\n",
        "    json.dump(description, fp)\n",
        "\n",
        "print(f\"Description file saved at {dataset_to_process['save_path']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb3d410-d52b-4273-b025-b3f29e367659",
      "metadata": {
        "id": "ddb3d410-d52b-4273-b025-b3f29e367659",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Interactively allows to select patient(s) to process\n",
        "# List the patients in the root folder:\n",
        "patients = list_folder_content(studypath)\n",
        "print('Patient data folders:')\n",
        "display_folder_list(patients)\n",
        "if not BATCHMODE:\n",
        "    # # Pick up an option:\n",
        "    patientIDX = input(f'Pick up a valid index to select a patient {tuple(range(len(patients)))} or type \"a\" to process all:')\n",
        "    if patientIDX == 'a':\n",
        "        print(f'Will process all patients in the test folder')\n",
        "    else:\n",
        "        patientIDX = int(patientIDX)\n",
        "        patients = [patients[patientIDX]]\n",
        "        if DEBUGMODE:\n",
        "            print(f'Patient {patients[0]} selected contains the follow datasets:')\n",
        "            print(display_tree(os.path.join(studypath, patients[0]), header=True, string_rep=True, show_hidden=False, max_depth=4))\n",
        "else:\n",
        "    print(f'Processing the whole data folder {studypath} as a batch process \\n***Please be patient!!***')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4a715c1-9303-4a4d-bc22-d0574d6b6c64",
      "metadata": {
        "id": "c4a715c1-9303-4a4d-bc22-d0574d6b6c64"
      },
      "source": [
        "It is expected the patient folder contains only 2 sub-folders:\n",
        "* PatientName-Pre-Treatment-\\<visit-date\\>\n",
        "* PatientName-Post-Treatment-\\<visit-date\\>\n",
        "\n",
        "and inside each of these sub-folders, there is the sequence number and the timepoints, where the corresponding Nifti file lives:\n",
        "```\n",
        "    PatientName-<Pre/Post>-Treatment-<visit-date>/\n",
        "    ├── SeqNro/\n",
        "        ├── 1/\n",
        "            ├── <SeqNro>_<sequence-name>.nii.gz\n",
        "        ├── 2/\n",
        "            ├── <SeqNro>_<sequence-name>.nii.gz\n",
        "        ├── 3/\n",
        "            ├── <SeqNro>_<sequence-name>.nii.gz\n",
        "        ├── 4/\n",
        "            ├── <SeqNro>_<sequence-name>.nii.gz\n",
        "        ├── 5/\n",
        "            ├── <SeqNro>_<sequence-name>.nii.gz\n",
        "        ├── 6/\n",
        "            ├── <SeqNro>_<sequence-name>.nii.gz\n",
        "```\n",
        "Note that for each timepoint, there is no difference in the Nifti file name, it is only differentiated by the folder enclosing it"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0fcffc6",
      "metadata": {
        "id": "d0fcffc6"
      },
      "source": [
        "## Checks the dataset are ok\n",
        "\n",
        "Review each sub-folder contains the right file and named correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f71596b-a9e8-4e27-a0cb-6d7414a1702d",
      "metadata": {
        "id": "5f71596b-a9e8-4e27-a0cb-6d7414a1702d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Organise the code to run for a single dataset and then, embed it into a loop for batch processing...\n",
        "# dataset_to_process = {}\n",
        "for patient in patients:\n",
        "    # loop over patient datafolder (pre- and post-treatment)\n",
        "    data_patient = os.path.join(studypath, patient)\n",
        "    dataset_to_process['datasets'][patient]={'output_path': patient, # data_patient.replace(studypath, savepath),\n",
        "                                 'visits': {}}\n",
        "    patient_visits = list_folder_content(data_patient)\n",
        "    checks = True\n",
        "    if DEBUGMODE:\n",
        "        print(f'Subfolders inside {data_patient}:\\n\\t{patient_visits}')\n",
        "    print(''.join(['*']*50))\n",
        "    for patient_visit in patient_visits:\n",
        "        visit_name = patient_visit.split('-')\n",
        "        seq_path = os.path.join(data_patient, patient_visit)\n",
        "        seq_nro = list_folder_content(seq_path)[0]\n",
        "        dataset_to_process['datasets'][patient]['visits'][''.join(visit_name[1:3])] = {'path': os.path.join(patient_visit, seq_nro),\n",
        "                                                                           'path2fixed': ''}\n",
        "        # Check the number of subfolders and depth are the expected ones:\n",
        "        print(f'Checking {seq_path} contains only 1 folder...')\n",
        "        check_nsequences_per_visit = check_time_points(seq_path, nmax=1, verbose=DEBUGMODE)\n",
        "        if check_nsequences_per_visit is not None:\n",
        "            print(f'Checking {seq_nro} contains the expected number of timepoints ...')\n",
        "            check_timepoints_per_seq = check_time_points(os.path.join(seq_path, seq_nro), verbose=DEBUGMODE)\n",
        "        else:\n",
        "            checks = False\n",
        "\n",
        "        if check_timepoints_per_seq is not None:\n",
        "            dce_tpoints_path = list_folder_content(os.path.join(seq_path, seq_nro))\n",
        "            dataset_to_process['datasets'][patient]['visits'][''.join(visit_name[1:3])]['path2moving'] = [None]*len(dce_tpoints_path)\n",
        "            for time_point_i in dce_tpoints_path:\n",
        "                print(f'Checking there is only one NIFTI file for each timepoint in {seq_nro}...')\n",
        "                check_nfiles_per_timepoint = check_time_points(os.path.join(seq_path, seq_nro, time_point_i), nmax=1)\n",
        "                get_nii_file = glob.glob(os.path.join(seq_path, seq_nro, time_point_i,'*.nii.gz'))\n",
        "                if (check_nfiles_per_timepoint is not None) and (len(get_nii_file)==1):\n",
        "                    print('Ready to load data...')\n",
        "                    if int(time_point_i) == fixed_volume_pos:\n",
        "                        dataset_to_process['datasets'][patient]['visits'][''.join(visit_name[1:3])]['path2fixed'] = get_nii_file[0]\n",
        "                    dataset_to_process['datasets'][patient]['visits'][''.join(visit_name[1:3])]['path2moving'][int(time_point_i)-1] = get_nii_file[0]\n",
        "                else:\n",
        "                    checks = False\n",
        "        else:\n",
        "            checks = False\n",
        "\n",
        "        if not checks:\n",
        "            print(f'***ERROR***: There is something wrong with the data in {seq_path}, please check!!')\n",
        "        print(''.join(['*']*100))\n",
        "    if checks:\n",
        "        print(f'All done loading NIFTI files from dataset {data_patient}')\n",
        "    else:\n",
        "        print(f'***ERROR***: There is something wrong with some (or all) data in {data_patient}, please check!!')\n",
        "    print(''.join(['*']*100))\n",
        "\n",
        "if DEBUGMODE:\n",
        "    print(f'Details of the datasets to process:')\n",
        "    print(json.dumps(dataset_to_process, indent=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75844c1b",
      "metadata": {
        "id": "75844c1b"
      },
      "source": [
        "## Load registration Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15afda4-516e-4fd5-b001-25d7861b8724",
      "metadata": {
        "id": "c15afda4-516e-4fd5-b001-25d7861b8724"
      },
      "outputs": [],
      "source": [
        "if dataset_to_process['registration_details']['algorithm'].lower() == 'ants':\n",
        "    # ANTs\n",
        "    # For details about possible values and description of parameters, see the help page: https://antspy.readthedocs.io/en/latest/registration.html\n",
        "    # Default values (as listed in the hep page)\n",
        "    dataset_to_process['par_set'] = {'type_of_transform': 'SyN',\n",
        "                                     'initial_transform': None,\n",
        "                                     'outprefix': '',\n",
        "                                     'mask': None,\n",
        "                                     'moving_mask': None,\n",
        "                                     'mask_all_stages': False,\n",
        "                                     'grad_step': 0.2,\n",
        "                                     'flow_sigma': 3,\n",
        "                                     'total_sigma': 0,\n",
        "                                     'aff_metric': 'mattes',\n",
        "                                     'aff_sampling': 32,\n",
        "                                     'aff_random_sampling_rate': 0.2,\n",
        "                                     'syn_metric': 'mattes',\n",
        "                                     'syn_sampling': 32,\n",
        "                                     'reg_iterations': (40, 20, 0),\n",
        "                                     'aff_iterations': (2100, 1200, 1200, 10),\n",
        "                                     'aff_shrink_factors': (6, 4, 2, 1),\n",
        "                                     'aff_smoothing_sigmas': (3, 2, 1, 0),\n",
        "                                     'write_composite_transform': False,\n",
        "                                     'random_seed': None\n",
        "                                    }\n",
        "\n",
        "    # To ensure reproducibility of the results, set the random_seed to a constant value:\n",
        "    dataset_to_process['par_set']['random_seed'] = 42 #(just to keep along with the pop-culture reference, e.g. https://medium.com/geekculture/the-story-behind-random-seed-42-in-machine-learning-b838c4ac290a\n",
        "    # Save the parameters as a JSON file in the parameters folder:\n",
        "    with open(os.path.join(dataset_to_process['save_path'], dataset_to_process['parameters_folder'], config_files[0]), 'w') as fp:\n",
        "        json.dump(dataset_to_process['par_set'], fp)\n",
        "elif dataset_to_process['registration_details']['algorithm'].lower() == 'elastix':\n",
        "    # Elastix\n",
        "    if DEBUGMODE:\n",
        "        print(f'Define the parameters for the registration. Please wait...')\n",
        "    dataset_to_process['par_set'] = itk.ParameterObject.New()\n",
        "    for par_files in sorted(dataset_to_process['registration_details']['configuration_files']):\n",
        "        dataset_to_process['par_set'].AddParameterFile(os.path.join(configpath, par_files))\n",
        "        # Copy the parameters files to the output folder:\n",
        "        shutil.copy2(os.path.join(configpath, par_files), os.path.join(dataset_to_process['save_path'],\n",
        "                                                                       dataset_to_process['parameters_folder'],\n",
        "                                                                       par_files))\n",
        "\n",
        "    if DEBUGMODE:\n",
        "        print('Parameters for ELASTIX:')\n",
        "        print(dataset_to_process['par_set'])\n",
        "else:\n",
        "    print(f\"Registration algorithm {dataset_to_process['registration_details']['algorithm']} not yet implemented. Please try again with a different option\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mS-rcWzBwy14",
      "metadata": {
        "id": "mS-rcWzBwy14"
      },
      "source": [
        "Loop over the patientes to run the registration --> ```list(dataset_to_process['datasets'].keys())```\n",
        "\n",
        "The workflow inside the loop is as follows:\n",
        "* Loop over the visits  --> ```list(dataset_to_process['datasets'][list(dataset_to_process['datasets'].keys())[i]]['visits'].keys())```\n",
        "* Loads the Nifti file asigned to the fixed volume:\n",
        "  * Initialises the 4D volume\n",
        "  * Loops over the moving dataset --> ```dataset_to_process['datasets'][list(dataset_to_process['datasets'].keys())[i]]['visits'][list(dataset_to_process['datasets'][list(dataset_to_process['datasets'].keys())[j]]['visits'].keys())[z]]['path2moving']```\n",
        "  * Loads the Nifti file\n",
        "  * ```\n",
        "        if moving_index == fixed_index:\n",
        "            skips registration and assigns the output to aux\n",
        "        else:\n",
        "            runs registration\n",
        "    ```\n",
        "  * saves the output data\n",
        "  * concatenate to 4D volume"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e0dd3a5",
      "metadata": {
        "id": "5e0dd3a5"
      },
      "source": [
        "## Finally, register all dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d4d93d3-ff82-48e7-9a24-65a9973ed761",
      "metadata": {
        "id": "3d4d93d3-ff82-48e7-9a24-65a9973ed761",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(f'Time at start: {time.ctime()}')\n",
        "init_time = time.perf_counter()\n",
        "for patient in dataset_to_process['datasets'].keys():\n",
        "  start_registering_patient = time.perf_counter()\n",
        "  for patient_visit in dataset_to_process['datasets'][patient]['visits']:\n",
        "      start_registering_visit = time.perf_counter()\n",
        "      patient_visit_outputpath = os.path.join(dataset_to_process['data_path'], patient, dataset_to_process['datasets'][patient]['visits'][patient_visit]['path'])\n",
        "      print(patient_visit_outputpath)\n",
        "      path2fixed = dataset_to_process['datasets'][patient]['visits'][patient_visit]['path2fixed']\n",
        "      print(f'Fixed Volume: {path2fixed}')\n",
        "      print(''.join(['-']*100))\n",
        "      fixed_volume = itk.imread(path2fixed)\n",
        "      fixed_voume_type = type(fixed_volume)\n",
        "      if bias_correction is not None:\n",
        "          # Apply the N4ITK correction to the fixed image\n",
        "          print('Applying N4ITK correction to the fixed volume...')\n",
        "          # fixed_volume_small = itk.shrink_image_filter(fixed_volume, shrink_factors=[2.0] * fixed_volume.GetImageDimension())\n",
        "          # corrector = itk.N4BiasFieldCorrectionImageFilter.New(fixed_volume_small)\n",
        "          # corrector.Update()\n",
        "          fixed_volume_SS3 = itk.N4BiasFieldCorrectionImageFilter(fixed_volume)\n",
        "          cast_filter_fixed_volume  = itk.CastImageFilter[type(fixed_volume_SS3), fixed_voume_type].New()\n",
        "          cast_filter_fixed_volume.SetInput(fixed_volume_SS3)\n",
        "          fixed_volume = cast_filter_fixed_volume.GetOutput()\n",
        "          # fixed_volume.Update()\n",
        "          # log_bias_field_fixed = fixed_volume.ReconstructBiasField(corrector.GetLogBiasFieldControlPointLattice())\n",
        "          # itk.imwrite(log_bias_field, moving_set.replace(dataset_to_process['study_path'], dataset_to_process['data_path']))\n",
        "\n",
        "\n",
        "      if dataset_to_process['registration_details']['algorithm'].lower() == 'ants':\n",
        "          # Convert ITK image to ANTs:\n",
        "          fixed_volume_ants = itk2ants(fixed_volume)\n",
        "          # Test whether I need to separate the Bias Correction and use the one implemented in ANTsPy\n",
        "          # https://antspy.readthedocs.io/en/latest/utils.html#ants.n4_bias_field_correction\n",
        "          # ouput = ants.n4_bias_field_correction(input)\n",
        "\n",
        "      # Get the list of moving datasets:\n",
        "      moving_datasets = dataset_to_process['datasets'][patient]['visits'][patient_visit]['path2moving']\n",
        "\n",
        "      # ITK concatenation output: Defines the 4D volume from this fixed image\n",
        "      # To stack the volumes, use the function TileFilter, following the example at\n",
        "      # https://examples.itk.org/src/filtering/imagegrid/create3dvolume/documentation\n",
        "      # However, as I found out the hard way, the method SetInput works in order, even if it is within a loop, it works lexicographically (i.e. ordinal numbers)\n",
        "      # so we'll recycle the list required for ANTs and after the registration, populates the tile in another (much quicker) loop\n",
        "      input_dimension = fixed_volume.GetImageDimension()\n",
        "      pixel_type = itk.template(fixed_volume)[1][0]\n",
        "      output_dimension = input_dimension + 1\n",
        "      input_image_type = itk.Image[pixel_type, input_dimension]\n",
        "      output_image_type = itk.Image[pixel_type, output_dimension]\n",
        "      layout = [1, 1, 1, len(moving_datasets)]\n",
        "      registered_tiles = itk.TileImageFilter[input_image_type, output_image_type].New()\n",
        "      registered_tiles.SetLayout(layout)\n",
        "\n",
        "\n",
        "      for idx, moving_set in enumerate(moving_datasets):\n",
        "          # Create the sub-folder in the output directories:\n",
        "          os.makedirs(os.path.join(patient_visit_outputpath, str(idx+1)), exist_ok=True)\n",
        "          # If the index is the Fixed Volume, just copy it to the output folder:\n",
        "          if ((idx+1) == fixed_volume_pos) and (not register_fixed):\n",
        "              if (bias_correction):\n",
        "                  # If bias field was applied, then we have to save the corrected image, instead of just copying the raw\n",
        "                  itk.imwrite(fixed_volume, path2fixed.replace(dataset_to_process['study_path'], dataset_to_process['data_path']))\n",
        "              else:\n",
        "                  print(f'No registration needed, {moving_set} is the fixed volume')\n",
        "                  shutil.copy2(path2fixed, path2fixed.replace(dataset_to_process['study_path'], dataset_to_process['data_path']))\n",
        "              registered_tiles.SetInput(idx, fixed_volume)\n",
        "          else:\n",
        "              # Any other case, just run the registration algorithm\n",
        "              print(f'Registering dataset {moving_set} to reference volume {path2fixed}. Please wait...')\n",
        "              start_registration_run = time.perf_counter()\n",
        "              moving_volume = itk.imread(moving_set)\n",
        "              moving_volume_type = type(moving_volume)\n",
        "              if bias_correction is not None:\n",
        "                  # Apply the N4ITK correction to the moving image\n",
        "                  print('Applying N4ITK correction to the moving volume...')\n",
        "                  # moving_volume_small = itk.shrink_image_filter(moving_volume, shrink_factors=[2.0] * moving_volume.GetImageDimension())\n",
        "                  # mov_corrector = itk.N4BiasFieldCorrectionImageFilter.New(moving_volume_small)\n",
        "                  # mov_corrector.Update()\n",
        "                  moving_volume_SS3 = itk.N4BiasFieldCorrectionImageFilter(moving_volume)\n",
        "                  cast_filter_moving_volume  = itk.CastImageFilter[type(moving_volume_SS3), moving_volume_type].New()\n",
        "                  cast_filter_moving_volume.SetInput(moving_volume_SS3)\n",
        "                  # cast_filter_moving_volume.Update()\n",
        "                  moving_volume = cast_filter_moving_volume.GetOutput()\n",
        "                  # log_bias_field_moving = fixed_volume.ReconstructBiasField(mov_corrector.GetLogBiasFieldControlPointLattice())\n",
        "              if dataset_to_process['registration_details']['algorithm'].lower() == 'elastix':\n",
        "                  if histogram_matching: # ANTsPy has this as default (see comment above)\n",
        "                      # Apply histogram matching between fixed and moving\n",
        "                      print('Applying histogram matching between fixed and moving images...')\n",
        "                      moving_volume = itk.HistogramMatchingImageFilter(moving_volume, fixed_volume)\n",
        "                  print('Running registration algorithm...')\n",
        "                  warped_moving, result_transform_pars = itk.elastix_registration_method(fixed_volume ,\n",
        "                                                                                          moving_volume,\n",
        "                                                                                          parameter_object=dataset_to_process['par_set'],\n",
        "                                                                                          log_to_console=False)\n",
        "              elif dataset_to_process['registration_details']['algorithm'].lower() == 'ants':\n",
        "                  print('Running registration algorithm...')\n",
        "                  moving_volume_ants = itk2ants(moving_volume)\n",
        "                  if histogram_matching:\n",
        "                      print('Applying histogram matching between fixed and moving images...')\n",
        "                      moving_volume_ants = ants.histogram_match_image(moving_volume_ants, fixed_volume_ants)\n",
        "                  registeredOutput = ants.registration(fixed=fixed_volume_ants , moving=moving_volume_ants, **dataset_to_process['par_set'])\n",
        "                  warped_moving = ants2itk(registeredOutput['warpedmovout'])\n",
        "\n",
        "\n",
        "              end_registration_run = time.perf_counter()\n",
        "              elp_registration_single = end_registration_run - start_registration_run\n",
        "              print(f'Elapsed time to register single volume (incl. loading the data): {elp_registration_single:0.2f}[s] ({timedelta(seconds=elp_registration_single)})')\n",
        "              print(f'Adding registered volume to the 4D tile...')\n",
        "              registered_tiles.SetInput(idx, warped_moving)\n",
        "              print(f\"Saving the output result in {moving_set.replace(dataset_to_process['study_path'], dataset_to_process['data_path'])}\")\n",
        "              itk.imwrite(warped_moving, moving_set.replace(dataset_to_process['study_path'], dataset_to_process['data_path']))\n",
        "              print(f'Finished registering timepoint {idx+1}')\n",
        "          print(''.join(['=']*100))\n",
        "      end_registering_visit = time.perf_counter()\n",
        "      elp_registration_visit = end_registering_visit - start_registering_visit\n",
        "      print(f'Elapsed time to register all timepoints in a visit (incl. loading the data): {elp_registration_visit:0.2f}[s] ({timedelta(seconds=elp_registration_visit)})')\n",
        "\n",
        "      if BATCHMODE:\n",
        "          # When running in BatchMode adds a small pause to avoid the \"IOStream.flush timed out\" error\n",
        "          print('Just breathing a little...')\n",
        "          time.sleep(5)\n",
        "          print('Ready to continue!')\n",
        "\n",
        "      # Saving the 4D time series :\n",
        "      start_saving_4Dvol = time.perf_counter()\n",
        "      reg_writer = itk.ImageFileWriter[output_image_type].New()\n",
        "      reg_writer.SetFileName(os.path.join(os.path.split(patient_visit_outputpath)[0],\n",
        "                                          '.'.join([os.path.split(dataset_to_process['datasets'][patient]['visits'][patient_visit]['path'])[0],\n",
        "                                                    'nii.gz'])))\n",
        "      reg_writer.SetInput(registered_tiles.GetOutput())\n",
        "      reg_writer.Update()\n",
        "\n",
        "      end_saving_4Dvol = time.perf_counter()\n",
        "      elp_saving_4Dvol = end_saving_4Dvol - start_saving_4Dvol\n",
        "      print(f'Elapsed time to save the 4D volume: {elp_saving_4Dvol:0.2f}[s] ({timedelta(seconds=elp_saving_4Dvol)})')\n",
        "\n",
        "      print(''.join(['*']*100))\n",
        "  end_registering_patient = time.perf_counter()\n",
        "  elp_registration_patient = end_registering_patient - start_registering_patient\n",
        "  print(f'Elapsed time to register a whole patient dataset: {elp_registration_patient:0.2f}[s] ({timedelta(seconds=elp_registration_patient)})')\n",
        "  print(''.join(['§']*100))\n",
        "\n",
        "final_time = time.perf_counter()\n",
        "elp_whole_loop = final_time - init_time\n",
        "print(f'Elapsed time to register the complete set of patientes in data folder {studypath}: {elp_whole_loop:0.2f}[s] ({timedelta(seconds=elp_whole_loop)})')\n",
        "print(f'Time at the end: {time.ctime()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d4eaa84",
      "metadata": {
        "id": "8d4eaa84"
      },
      "source": [
        "# Applies the ROI and extract signal intensities from the registered dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "198900fa",
      "metadata": {
        "id": "198900fa"
      },
      "outputs": [],
      "source": [
        "# Loads the ROI dictionary\n",
        "roi_dictionary = json.loads(open(os.path.join(subdir_paths['CFGFLDR'],'ROI_Dictionary.json')).read())\n",
        "path_to_landmarks = os.path.join(subdir_paths['ROIFLDR'], 'datasets')\n",
        "path_to_volumes = os.path.join(subdir_paths['REGOUTFLDR'], 'datasets')\n",
        "\n",
        "nrows = 2\n",
        "ncols = 2\n",
        "rows_width = 12.0\n",
        "cols_heigth = 10.0\n",
        "\n",
        "kernel = np.ones((1)*3,np.uint8)\n",
        "marker = itertools.cycle(['x','+','.','o','^','v'])\n",
        "\n",
        "landmark_sel = 'tt'\n",
        "# landmark_sel = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9593436",
      "metadata": {
        "id": "c9593436"
      },
      "outputs": [],
      "source": [
        "hdr = ['Patient','Patient_visit','Landmark', 'SliceNro','LandmarkIDX','time_course']# + [f't{tp:03d}' for tp in range(6)]\n",
        "df_row = []\n",
        "for patient in dataset_to_process['datasets'].keys():\n",
        "    start_registering_patient = time.perf_counter()\n",
        "    for patient_visit in dataset_to_process['datasets'][patient]['visits']:\n",
        "        path_to_roi = os.path.join(path_to_landmarks, patient, os.path.dirname(dataset_to_process['datasets'][patient]['visits'][patient_visit]['path']), 'landmarks', 'RoiSet-Frame002.zip')\n",
        "        # Load the image volume and see how can I overlay the ROI:\n",
        "        path_to_volume = os.path.join(path_to_volumes, patient,\n",
        "                                      os.path.dirname(dataset_to_process['datasets'][patient]['visits'][patient_visit]['path']),\n",
        "                                      '.'.join([os.path.split(dataset_to_process['datasets'][patient]['visits'][patient_visit]['path'])[0],\n",
        "                                                'nii.gz']))\n",
        "        if os.path.isfile(path_to_roi):\n",
        "            roi_zip_file = roifile.ImagejRoi.fromfile(path_to_roi)\n",
        "            volume_4d_file = itk.imread(path_to_volume)\n",
        "            # Using Matplotlib to display the images (for now skip the 3D visualisation issue with itk.view)\n",
        "            nrois = len(roi_zip_file)\n",
        "            nt = volume_4d_file.shape[0]\n",
        "            if landmark_sel == '':\n",
        "                ncols = nrois\n",
        "            else:\n",
        "                ncols = 1\n",
        "            si_landmark = {}\n",
        "\n",
        "            fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(rows_width, cols_heigth), sharey='row')\n",
        "            fig.suptitle(os.path.dirname(dataset_to_process['datasets'][patient]['visits'][patient_visit]['path']))\n",
        "            tcourse_ax = fig.add_subplot(2,1,2)\n",
        "            axr = ax.ravel()\n",
        "\n",
        "            for idx_roi, roi in enumerate(roi_zip_file):\n",
        "                coordinates = (roi.t_position - 1, roi.z_position - 1) # Indices start in 0, but in ImageJ, slice and time points start in 1\n",
        "                [slice_nro, locID, landmrkID, landmarkIDX] = roi.name.split('-')\n",
        "                location = roi_dictionary['locations'][locID.lower()].capitalize()\n",
        "                landmark = roi_dictionary['landmarks'][landmrkID.lower()].capitalize()\n",
        "                if landmark_sel == '':\n",
        "                    img_ax = ax[0, idx_roi]# img_ax = ax[idx_roi, 0]\n",
        "                else:\n",
        "                    img_ax = axr[0]\n",
        "\n",
        "                if (landmrkID.lower() == landmark_sel) | (landmark_sel == ''):\n",
        "                    time_courses = [patient, patient_visit, landmark, slice_nro, landmarkIDX]\n",
        "                    slice_to_plot = volume_4d_file[coordinates[0],coordinates[1],::-1,:]\n",
        "                else:\n",
        "                    continue\n",
        "                img_ax.imshow(slice_to_plot, cmap='gray')\n",
        "                img_ax.set_title(roi.name)\n",
        "                roi.plot(ax=img_ax, linewidth=1, color='y')\n",
        "                # img_ax.axis([275, 450, 300, 70])\n",
        "                img_ax.axis('off')\n",
        "                # Create a mask from the set of coordinates:\n",
        "                mask = np.zeros_like(slice_to_plot, dtype=np.uint8)\n",
        "                pts = roi.coordinates().round().astype(np.int32)\n",
        "                mask = cv2.fillPoly(mask, [pts], (1))\n",
        "                mask = cv2.erode(mask, kernel, iterations=1)\n",
        "\n",
        "                si = [np.mean(np.asarray(volume_4d_file[ti,coordinates[1],::-1,:] * mask), where=(mask>0)) for ti in range(volume_4d_file.shape[0])]\n",
        "                pc_si = list(si/(si[0]/100))\n",
        "                roi_label = f\"{landmark}_{slice_nro}_IDX{int(landmarkIDX):02d}\"\n",
        "                si_landmark[roi_label] = pc_si\n",
        "                if landmark_sel == '':\n",
        "                    plt_ax = ax[1, idx_roi]\n",
        "                else:\n",
        "                    plt_ax = axr[1]\n",
        "                plt_ax.remove()\n",
        "                time_courses.append(pc_si)\n",
        "                time_course_df = pd.DataFrame(si_landmark)\n",
        "                df_row.append(time_courses)\n",
        "                tcourse_ax.plot(pc_si, marker=next(marker), label=roi_label)\n",
        "                if landmrkID.lower() == landmark_sel:\n",
        "                    # get stats of the timecourse signal:\n",
        "                    t0 = 1\n",
        "                    t_axis = range(t0, nt)\n",
        "                    y_hat = pc_si[t0:]\n",
        "                    first_pass_enh = pc_si[1]-pc_si[0]\n",
        "                    max_uptk = np.max(pc_si)\n",
        "                    delta_enh = pc_si[-1] - pc_si[t0]\n",
        "                    p = np.polyfit(t_axis, y_hat, 1)\n",
        "                    tcourse_ax.text(t_axis[-2], 0.90*pc_si[-2], f'Max Uptake: {max_uptk:0.1f}%')\n",
        "                    tcourse_ax.text(t0, 0.70*pc_si[t0], r'$\\Delta$ ENH:' + f'{delta_enh:0.1f}%')\n",
        "                    tcourse_ax.text(t0, 0.80*pc_si[t0], f'Slope: {p[0]:0.1f}')\n",
        "                    tcourse_ax.text(t0, 0.90*pc_si[t0], f'1st Pass: {first_pass_enh:0.1f}%')\n",
        "                    tcourse_ax.plot(t_axis, np.polyval(p, t_axis), label='Linear Fit')\n",
        "                    tcourse_ax.set_ylim([50, 300])\n",
        "                    # tcourse_ax.grid(True)\n",
        "            tcourse_ax.set_title(f'Percentage of Signal Intensity over time')\n",
        "            tcourse_ax.set_xlabel(f'Time Points')\n",
        "            tcourse_ax.set_ylabel(r'$\\frac{SI(t)}{SI(0)}$[%]')\n",
        "            tcourse_ax.legend()\n",
        "\n",
        "            plt.savefig(os.path.join(subdir_paths['OUTPUTFLDR'], '.'.join([os.path.dirname(dataset_to_process['datasets'][patient]['visits'][patient_visit]['path']),\n",
        "                                                                           'png'])))\n",
        "            plt.show()\n",
        "\n",
        "df = pd.DataFrame(columns=hdr, data=df_row)\n",
        "df.head()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}